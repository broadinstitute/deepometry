{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import numpy\n",
    "from sklearn.preprocessing import scale\n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "import deepometry.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "After training a model to classify single cell images, it is often useful to evaluate the performance of the model on an unseen annotated dataset. Evaluation helps predict model performance on unseen data.\n",
    "\n",
    "Suppose we have the following directory structure where images from one experiment (`experiment_02`) have been classified as one of three classes (`class_A`, `class_B`, or `class_C`). Data from this experiment was not shown to the model during training. Images are saved as NPY files with patient prefixes:\n",
    "\n",
    "    /data/parsed/\n",
    "        experiment_02/\n",
    "            class_A/\n",
    "                patient_A__32e88e1ac3a8f44bf8f77371155553b9.npy\n",
    "                patient_A__3dc56a0c446942aa0da170acfa922091.npy  \n",
    "                ...\n",
    "            class_B/\n",
    "                patient_A__8068ef7dcddd89da4ca9740bd2ccb31e.npy\n",
    "                patient_A__8348deaa70dfc95c46bd02984d28b873.npy\n",
    "                ...\n",
    "            class_C/  \n",
    "                patient_A__c1ecbca7bd98c01c1d3293b64cd6739a.npy\n",
    "                patient_A__c56cfb8e7e7121dd822e47c67d07e2d4.npy\n",
    "                ...\n",
    "                \n",
    "\n",
    "The data can be used to evaluate a model for classifying image data as one of the three classes. The `collect_pathnames` and `load` functions defined below will select images to use for evaluating the model and generate the labels for the evaluation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_pathnames(directories, labels):\n",
    "    \"\"\"\n",
    "    :param directories: List of directories to select samples from. Assumes subdirectories of each directory\n",
    "                        correspond to class labels. Contents of subdirectories are NPY files containing data\n",
    "                        of that label.\n",
    "    :return: List of pathnames.\n",
    "    \"\"\"\n",
    "    pathnames = []\n",
    "\n",
    "    for directory in directories:\n",
    "        subdirectories = sorted(glob.glob(os.path.join(directory, \"*\")))\n",
    "        \n",
    "        # transform the files of the same label into directory\n",
    "        subdirectory_pathnames = [glob.glob(\"{}/*.npy\".format(subdirectory)) for subdirectory in subdirectories ]      \n",
    "\n",
    "        nsamples = max([len(pathnames) for pathnames in subdirectory_pathnames])\n",
    "        print(nsamples)\n",
    "\n",
    "        pathnames += [list(numpy.random.permutation(pathnames)[:nsamples]) for pathnames in subdirectory_pathnames]\n",
    "\n",
    "    pathnames = sum(pathnames, [])\n",
    "\n",
    "    return pathnames\n",
    "\n",
    "\n",
    "def load(pathnames, labels, patient_to_include):\n",
    "    \"\"\"\n",
    "    Load training and target data.\n",
    "    \n",
    "    Assumes data is stored in a directory corresponding to some class label.\n",
    "\n",
    "    :param pathnames: List of image pathnames.\n",
    "    :param labels: List of class labels.\n",
    "    :return: Tuple (training, target) data, as NumPy arrays.\n",
    "    \"\"\"\n",
    "    print('Before exclusion: ',len(pathnames))\n",
    "    pathnames = [x for x in pathnames if patient_to_include in x]\n",
    "    print('After exclusion: ',len(pathnames))\n",
    "\n",
    "    x = numpy.empty((len(pathnames),) + _shape(pathnames[0]), dtype=numpy.uint8)\n",
    "\n",
    "    y = numpy.empty((len(pathnames),), dtype=numpy.uint8)\n",
    "\n",
    "    label_to_index = {label: index for index, label in enumerate(sorted(labels))}\n",
    "\n",
    "    for index, pathname in enumerate(pathnames):\n",
    "        if (os.path.isfile(pathname) == True):\n",
    "\n",
    "            label = os.path.split(os.path.dirname(pathname))[-1]\n",
    "\n",
    "            x[index] = numpy.load(pathname)\n",
    "\n",
    "            y[index] = label_to_index[label]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def _shape(pathname):\n",
    "    \"\"\"\n",
    "    Infer the shape of the sample data from a single sample.\n",
    "    \n",
    "    :param pathname: Path to a sample.\n",
    "    :return: Sample dimensions.\n",
    "    \"\"\"\n",
    "    return numpy.load(pathname).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\"/data/parsed/\"]\n",
    "\n",
    "labels = [\"class_A\", \"class_B\", \"class_C\"]\n",
    "\n",
    "pathnames = collect_pathnames(directories, labels)\n",
    "\n",
    "patient_to_test = 'patient_A'\n",
    "x, y = load(pathnames, labels, patient_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build session running on GPU 1\n",
    "configuration = tensorflow.ConfigProto()\n",
    "configuration.gpu_options.allow_growth = True\n",
    "configuration.gpu_options.visible_device_list = \"3\"\n",
    "session = tensorflow.Session(config = configuration)\n",
    "\n",
    "# apply session\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = deepometry.model.Model(shape=x.shape[1:], units=4)\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.load_weights('/model/resnet/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation and target data (`x` and `y`, respectively) is next passed to the model for evaluation. **A previously trained model is required.** The `evaluate` method loads the trained model weights. See the `fit` notebook for instructions on training a model. \n",
    "\n",
    "Evaluation data is provided to the model in batches of 32 samples. Use `batch_size` to configure the number of samples. A smaller `batch_size` requires less memory.\n",
    "\n",
    "The evaluate function outputs the model's loss and accuracy metrics as the array `[loss, accuracy]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(\n",
    "    batch_size=50,\n",
    "    x=x\n",
    ")\n",
    "\n",
    "predicted = numpy.argmax(predicted, -1)\n",
    "expected = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import sklearn.metrics\n",
    "import pandas\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(expected, predicted)\n",
    "\n",
    "confusion = pandas.DataFrame(confusion)\n",
    "\n",
    "matplotlib.pyplot.figure(figsize=(12, 8))\n",
    "\n",
    "seaborn.heatmap(confusion, annot=True)\n",
    "\n",
    "#output_directory = '/results/'\n",
    "#matplotlib.pyplot.savefig( os.path.join(output_directory, 'confusion_matrix_absolute.eps') , format='eps', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(expected, predicted)\n",
    "\n",
    "confusion = confusion.astype('float') / confusion.sum(axis=1)[:, numpy.newaxis]\n",
    "\n",
    "confusion = pandas.DataFrame(confusion)\n",
    "\n",
    "matplotlib.pyplot.figure(figsize=(12, 8))\n",
    "\n",
    "seaborn.heatmap(confusion, annot=True)\n",
    "\n",
    "#output_directory = '/results/'\n",
    "#matplotlib.pyplot.savefig( os.path.join(output_directory, 'confusion_matrix_percent.eps') , format='eps', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(expected, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For unsupervised t-SNE/PCA\n",
    "\n",
    "to be used on http://projector.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_pathnames(directories, labels):\n",
    "    \"\"\"\n",
    "    :param directories: List of directories to select samples from. Assumes subdirectories of each directory\n",
    "                        correspond to class labels. Contents of subdirectories are NPY files containing data\n",
    "                        of that label.\n",
    "    :return: List of pathnames.\n",
    "    \"\"\"\n",
    "    pathnames = []\n",
    "\n",
    "    for directory in directories:\n",
    "        subdirectories = sorted(glob.glob(os.path.join(directory, \"*\")))\n",
    "        \n",
    "        # transform the files of the same label into directory\n",
    "        filelist = [glob.glob(\"{}/*{}*\".format(subdirectory,label)) for label in labels for subdirectory in subdirectories ]\n",
    "        \n",
    "        subdirectory_pathnames = []\n",
    "        for i in range(len(labels)):\n",
    "            a = filelist[i*len(subdirectories):(i+1)*len(subdirectories)]\n",
    "            subdirectory_pathnames.append( list(itertools.chain.from_iterable(a)) )        \n",
    "\n",
    "        #nsamples = max([len(pathnames) for pathnames in subdirectory_pathnames])\n",
    "        nsamples = 5000 # this is helpful to limit the number of datapoints to be displayed on projector.tensorflow.org\n",
    "\n",
    "        pathnames += [list(numpy.random.permutation(pathnames)[:nsamples]) for pathnames in subdirectory_pathnames]\n",
    "\n",
    "    pathnames = sum(pathnames, [])\n",
    "\n",
    "    return pathnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathnames = collect_pathnames(directories, labels)\n",
    "x, y = load(pathnames, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(layers[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract_model = None # Clear cached abstract_model\n",
    "abstract_model = Sequential([layers[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_features = abstract_model.predict(\n",
    "    batch_size=50,\n",
    "    x=x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.savetxt('/results/table_of_features.txt' , scale(extracted_features), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_metadata(file):\n",
    "    with open(file, 'w') as f:\n",
    "        for i in range(y.shape[0]):\n",
    "            f.write('{}\\n'.format( list(sorted(labels))[y[i]] ))     \n",
    "\n",
    "save_metadata('/results/metadata.tsv')\n",
    "\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
