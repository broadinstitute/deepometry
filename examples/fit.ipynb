{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(numpy.median([3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy\n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "import deepometry.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit\n",
    "\n",
    "After parsing raw .CIF files into numpy arrays, we expectingly have the following directory structure, where numpy arrays .NPY are saved in subdirectories named after the class labels (e.g. `positive`, `negative` etc.).\n",
    "\n",
    "    /data/\n",
    "        patient_A/\n",
    "            positive/\n",
    "                32e88e1ac3a8f44bf8f77371155553b9.npy\n",
    "                3dc56a0c446942aa0da170acfa922091.npy  \n",
    "                ...\n",
    "            negative/\n",
    "                8068ef7dcddd89da4ca9740bd2ccb31e.npy\n",
    "                8348deaa70dfc95c46bd02984d28b873.npy\n",
    "                ...\n",
    "        patient_B/\n",
    "            positive/\n",
    "                3618e715e62a229aa78a7e373b49b888.npy\n",
    "                3cf53cea7f4db1cfd101e06c366c9868.npy\n",
    "                ...\n",
    "            negative/\n",
    "                84949e1eba7802b00d4a1755fa9af15e.npy\n",
    "                852a1edbf5729fe8721e9e5404a8ad20.npy\n",
    "                ...\n",
    "\n",
    "                \n",
    "The data can be used to train a model to classify image data as one of the three classes. The `sample` and `load` functions defined below will select images to use for training the model and generate the labels for the training images.\n",
    "\n",
    "Suppose there is a large imbalance between the number of samples per class in each experiment. Undersampling across classes balances the data seen by the model during training. Additionally, `class_weights` are introduced to work together with undersampling to improve prediction accuracy in underrepresented classes. The `sample` function performs undersampling across classes (per-experiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _shape(pathname):\n",
    "    \"\"\"\n",
    "    Infer the shape of the sample data from a single sample.\n",
    "    \n",
    "    :param pathname: Path to a sample.\n",
    "    :return: Sample dimensions.\n",
    "    \"\"\"\n",
    "    return numpy.load(pathname).shape\n",
    "\n",
    "\n",
    "def load(pathnames, labels):\n",
    "    \"\"\"\n",
    "    Load training and target data.\n",
    "    \n",
    "    Assumes data is stored in a directory corresponding to some class label.\n",
    "\n",
    "    :param pathnames: List of image pathnames.\n",
    "    :param labels: List of class labels.\n",
    "    :return: Tuple (training, target) data, as NumPy arrays.\n",
    "    \"\"\"\n",
    "    x = numpy.empty((len(pathnames),) + _shape(pathnames[0]), dtype=numpy.uint8)\n",
    "\n",
    "    y = numpy.empty((len(pathnames),), dtype=numpy.uint8)\n",
    "\n",
    "    label_to_index = {label: index for index, label in enumerate(sorted(labels))}\n",
    "\n",
    "    for index, pathname in enumerate(pathnames):\n",
    "        if os.path.isfile(pathname) == True:\n",
    "\n",
    "            label = os.path.split(os.path.dirname(pathname))[-1]\n",
    "\n",
    "            x[index] = numpy.load(pathname)\n",
    "\n",
    "            y[index] = label_to_index[label]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(directories):\n",
    "    \"\"\"\n",
    "    Sample pathnames from directories. \n",
    "    \n",
    "    For each directory, samples are randomly selected equally across subdirectories.\n",
    "\n",
    "    :param directories: List of directories to select samples from. Assumes subdirectories of each directory\n",
    "                        correspond to class labels. Contents of subdirectories are NPY files containing data\n",
    "                        of that label.\n",
    "    :return: List of sampled pathnames.\n",
    "    \"\"\"\n",
    "    pathnames = []\n",
    "\n",
    "    for directory in directories:\n",
    "        subdirectories = sorted(glob.glob(os.path.join(directory, \"*\")))\n",
    "\n",
    "        subdirectory_pathnames = [glob.glob(os.path.join(subdirectory, \"*\")) for subdirectory in subdirectories]\n",
    "\n",
    "        nsamples = min([len(pathnames) for pathnames in subdirectory_pathnames])\n",
    "\n",
    "        pathnames += [list(numpy.random.permutation(pathnames)[:nsamples]) for pathnames in subdirectory_pathnames]\n",
    "\n",
    "    pathnames = sum(pathnames, [])\n",
    "\n",
    "    return pathnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directories = [\"/data/parsed/\"]\n",
    "\n",
    "# labels = [os.path.split(x[0])[-1] for x in os.walk(directories[0])][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['unactivated','siglecneg2','activated','siglecneg1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = sample(directories)\n",
    "\n",
    "x, y = load(samples, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and target data (`x` and `y`, respectively) is next passed to the model for training. The model is confiured to withhold 20% of the training data for validation. Use `validation_split` to adjust the size of the partition.\n",
    "\n",
    "The model will iterate over the training data at most 512 times, specified by `epochs`. Training will terminate early if the validation loss fails to improve for 20 epochs. Training and validation data is provided to the model in batches of 32 samples. Use `batch_size` to configure the number of samples. A smaller `batch_size` requires less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build session running on GPU 1\n",
    "configuration = tensorflow.ConfigProto()\n",
    "configuration.gpu_options.allow_growth = True\n",
    "configuration.gpu_options.visible_device_list = \"3\"\n",
    "session = tensorflow.Session(config = configuration)\n",
    "\n",
    "# apply session\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = deepometry.model.Model(shape=x.shape[1:], units=len(labels))\n",
    "\n",
    "model.compile()\n",
    "\n",
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=32,\n",
    "    epochs=512,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import pkg_resources\n",
    "\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = pandas.read_csv(pkg_resources.resource_filename(\"deepometry\", \"data/training.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, (ax0, ax1) = matplotlib.pyplot.subplots(ncols=2, figsize=(16, 4))\n",
    "\n",
    "ax0.plot(csv[\"acc\"], c=\"r\")\n",
    "ax0.plot(csv[\"val_acc\"], c=\"b\")\n",
    "\n",
    "ax1.plot(csv[\"loss\"][30:], c=\"r\")\n",
    "ax1.plot(csv[\"val_loss\"][30:], c=\"b\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
